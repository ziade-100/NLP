{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "987a0b16",
   "metadata": {},
   "source": [
    "<h1>What is Preprocessing</h1>\n",
    "<p>\n",
    "Preprocessing in the context of Natural Language Processing (NLP) refers to the steps taken to clean and prepare raw text data before it is used in a machine learning model. This involves transforming the text into a format that can be easily understood and analyzed by algorithms. The goal of preprocessing is to reduce noise, normalize the text, and convert it into numerical features that can be fed into machine learning models</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12087261",
   "metadata": {},
   "source": [
    "<h2>1. Steps of Preprocessing</h2>\n",
    "<ol>\n",
    "    <li>Lowercasing</li>\n",
    "    <li>Removing Punctuation</li>\n",
    "    <li>Tokenization</li>\n",
    "    <li>Removing Stop Words</li>\n",
    "    <li>Lemmatization</li> <ol>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73a9b9cc",
   "metadata": {},
   "source": [
    "#### 1.1  Lowercasing \n",
    "<p> \n",
    "   Converting all characters in the text to lowercase ensures that words like \"Depression\" and \"depression\" are treated as the same word\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "323f7748",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "# Load spaCy model\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "8bc0482d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process the text with spaCy\n",
    "text = \"I am Feeling Great Today!\"\n",
    "doc = nlp(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "b49cb753",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i am feeling great today!\n"
     ]
    }
   ],
   "source": [
    "# conveting text to lowercase\n",
    "lowercased_text = text.lower()\n",
    "\n",
    "print(lowercased_text)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bcf6e71",
   "metadata": {},
   "source": [
    "#### 1.2 Removing Punctuation\n",
    "<p>Remove punctuation marks (like periods, commas, and question marks) from the text since they usually do not contribute to the meaning in NLP tasks.\n",
    "\n",
    " </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "407b0c7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import string\n",
    "import string "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "b8dcdf56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example text\n",
    "text1 = \"What is NLP?\"\n",
    "text2 = \"Feeling Depressed and anxious.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "a7711ad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting to lower case\n",
    "text1_lower = text1.lower()\n",
    "text2_lower = text2.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "5fd102ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove Puctuations \n",
    "text1_no_punct = text1_lower.translate(str.maketrans('', '', string.punctuation))\n",
    "\n",
    "text2_no_punct = text2_lower.translate(str.maketrans('', '', string.punctuation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "bb28c0c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'what is nlp'"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text1_no_punct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "f702a22e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'feeling depressed and anxious'"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text2_no_punct"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7df7dc46",
   "metadata": {},
   "source": [
    "#### 1.3 Tokenization\n",
    "<p> <b>Tokenization</b> is the process of breaking down text into smaller units called tokens. In NLP, tokens are typically words, punctuation marks, or other meaningful elements. Tokenization is a crucial first step in many NLP tasks because it allows you to work with individual units of text.\n",
    "\n",
    " </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "350b7f6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example Text \n",
    "text = \"I am feeling great today! How about you?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "64cd9513",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lower case text\n",
    "text_lower = text.lower()\n",
    "\n",
    "# removing punctuations \n",
    "text_no_punct = text_lower.translate(str.maketrans('','',string.punctuation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "3e522476",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process the input text\n",
    "doc = nlp(text_no_punct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "377c9026",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i\n",
      "am\n",
      "feeling\n",
      "great\n",
      "today\n",
      "how\n",
      "about\n",
      "you\n"
     ]
    }
   ],
   "source": [
    "# iterating over tokens \n",
    "for token in doc:\n",
    "    print(token.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fcf83c3",
   "metadata": {},
   "source": [
    "#### 1.4 Removing Stop Words\n",
    "<p> Stpo Words are common words that do not carry significant meaning or information for the analysis. These words, known as stop words, are frequently occurring words such as \"the,\" \"is,\" \"and,\" \"in,\" etc. Removing stop words can help reduce noise in the text data.  </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "a607f384",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input text\n",
    "text_with_stop_words = \"I am feeling great today, but tomorrow might not be the same.\"\n",
    "\n",
    "# Process the text with spaCy\n",
    "doc = nlp(text_with_stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "2c6ae51b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter out stop words and join the remaining tokens\n",
    "filtered_text = ' '.join([token.text for token in doc if not token.is_stop])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "f5f2668e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'feeling great today , tomorrow .'"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print the filtered text\n",
    "filtered_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61b46636",
   "metadata": {},
   "source": [
    "#### 1.4 Lemmatization\n",
    "<p> \n",
    "  Lemmatization is the process of reducing words to their base or root form.  Lemmatization helps in normalizing words so that variations of the same word are treated as the same token, which can improve the performance of natural language processing tasks like text analysis, information retrieval, and machine learning.\n",
    "\n",
    "<h4>How Lemmatization Works</h4>\n",
    "\n",
    "<b>Tokenization</b> First, the text is tokenized into individual words or tokens.<br>\n",
    "<b>Part-of-Speech Tagging</b> Each token is assigned a part-of-speech tag to determine its grammatical category (noun, verb, adjective, etc.).<br>\n",
    "<b>Lemmatization </b> Based on the part-of-speech tag, each token is reduced to its base form (lemma). This involves removing inflections and suffixes to get to the root form of the word.\n",
    " </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "1f790ee8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input text\n",
    "text = \"I am running in the park.\"\n",
    "\n",
    "# Process the text with spaCy\n",
    "doc = nlp(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "4eedf8fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lemmatize each token and join the results\n",
    "lemmatized_text = ' '.join([token.lemma_ for token in doc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "69c2866e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I be run in the park .'"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatized_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2768d49",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
