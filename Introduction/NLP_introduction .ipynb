{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ed0626a6",
   "metadata": {},
   "source": [
    "### Natural Language Processing (NLP)\n",
    "<p>Natural Language Processing (NLP) is a branch of artificial intelligence concerned with enabling computers to understand, interpret, and generate human language in a manner that is both meaningful and contextually appropriate.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "797c7b77",
   "metadata": {},
   "source": [
    "### Applications of NLP includes \n",
    "<ol>\n",
    "  <li>Text Classification/Document Classification</li>\n",
    "  <li>Named Entity Recognition (NER)</li>\n",
    "  <li>Part-of-Speech Tagging (POS)</li>\n",
    "  <li>Sentiment Analysis</li>\n",
    "  <li>Text Summarization</li>\n",
    "  <li>Question Answering</li>\n",
    "  <li>Machine Translation</li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24a7e9fa",
   "metadata": {},
   "source": [
    "### Libraries for NLP \n",
    "<p> there are many important Libraires for NLP tasks and here I will use \n",
    "   <ul>\n",
    "       <li> NLTK </li>\n",
    "       <li> Spacy</li> \n",
    "       <li> Gensim</li>\n",
    "       <li> Transformers</li>\n",
    "       </ul>\n",
    "       </p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b475261",
   "metadata": {},
   "source": [
    "### Text Pre-processing \n",
    "<p>Text often has extra words and stuff we don't need. We have to tidy it before doing NLP tasks. Here's how\n",
    "<ol>\n",
    "    <li>Tokenization</li>\n",
    "    <li>Stopword Removal and Punctuation Removal</li>\n",
    "    <li>Stemming</li>\n",
    "    <li>Lemmatization</li>\n",
    "</ol>\n",
    "    </p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c7bccc1",
   "metadata": {},
   "source": [
    "### Feature Extraction \n",
    "<p> Once we preprocess the text we have to do feature extraction. Feature exctraction is  for detailed explanation of Feature extraction follow the following \n",
    "<a href=\"https://medium.com/@eskandar.sahel/exploring-feature-extraction-techniques-for-natural-language-processing-46052ee6514#:~:text=In%20natural%20language%20processing%20(NLP,processed%20by%20machine%20learning%20algorithms.\"> article</a> </p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2de94992",
   "metadata": {},
   "source": [
    "### spacy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8bc0482d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b49cb753",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bcf6e71",
   "metadata": {},
   "source": [
    "#### Tokenizations "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "11100cfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"As long as you Win it doesn't matter how\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7999b2ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = nlp(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c311c562",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "As long as you Win it does n't matter how "
     ]
    }
   ],
   "source": [
    "for token in tokens:\n",
    "    print(token,end=\" \")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7df7dc46",
   "metadata": {},
   "source": [
    "#### Stop words "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cf49edbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = nlp.Defaults.stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dc4eeb71",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "326"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(stop_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cf8b760",
   "metadata": {},
   "source": [
    "#### punctuations "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "5be7e73c",
   "metadata": {},
   "outputs": [],
   "source": [
    "punctuations = string.punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "c53fe6a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "punctuations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e074f73",
   "metadata": {},
   "source": [
    "### tokenizer function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "dd9b7fab",
   "metadata": {},
   "outputs": [],
   "source": [
    "#spacy tokenizer function\n",
    "def spacy_tokenizer(text):\n",
    "    # create tokens from text\n",
    "    doc = nlp(text)\n",
    "    \n",
    "    #change to token to it's Lemma\n",
    "    my_tokens = [token.lemma_.lower().strip() for token in doc]\n",
    "    \n",
    "    # remove puctuations and stop_words from tokens \n",
    "    my_tokens = [token for token in my_tokens if token not in stop_words and token not in punctuations]\n",
    "    return my_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "30fa512b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['text', 'collection', 'text']"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"Text and is collection of text.\"\n",
    "spacy_tokenizer(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5f2668e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "314d2171",
   "metadata": {},
   "source": [
    "### Vectorizers \n",
    "</br>\n",
    "explain what they are and why we need it"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fcae06b",
   "metadata": {},
   "source": [
    "we are going to use <b>CountVectorizer</b> form Skleanr Liberary \n",
    "which Convert a collection of text documents to a matrix of token counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "54962378",
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vectorizer = CountVectorizer(tokenizer=spacy_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "b704cf0e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 1, 1, 1]])"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d = \" collection of Text and things like!!\"\n",
    "count_vectorizer.fit_transform([d]).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "caedd756",
   "metadata": {},
   "outputs": [],
   "source": [
    "### get feature names "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "8e670ecb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['collection', 'like', 'text', 'thing'], dtype=object)"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_vectorizer.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "103b93a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'collection': 0, 'text': 2, 'thing': 3, 'like': 1}"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_vectorizer.vocabulary_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa3b832d",
   "metadata": {},
   "source": [
    "### with TF-IDF Vectorizer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "id": "ff4533a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_tokenizer= TfidfVectorizer(tokenizer=spacy_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "id": "a4b44407",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_train = tf_tokenizer.fit_transform(X_trian)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "id": "fdc656c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_test = tf_tokenizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "id": "639bf572",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(800, 2777)"
      ]
     },
     "execution_count": 244,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "id": "6c995532",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(200, 2777)"
      ]
     },
     "execution_count": 246,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "962d5b7a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
