{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dd5466e7",
   "metadata": {},
   "source": [
    "<h1> Vectorization</h1>\n",
    "<p> \n",
    "Vectorization is the process of converting textual data into numerical vectors, where each vector represents a piece of text such as a document or a sentence. This conversion allows machines to process and analyze text data using mathematical and statistical techniques.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46c68649",
   "metadata": {},
   "source": [
    "<h3>Techniques of Vectorization</h3>\n",
    "<p><b>Bag-of-Words (BoW)</b> : Represents each document as a vector where each dimension corresponds to a unique word in the corpus, and the value represents the frequency of that word in the document.</p>\n",
    "\n",
    "<p><b>TF-IDF (Term Frequency-Inverse Document Frequency)</b>\n",
    "    :Similar to BoW but also considers the importance of words based on their frequency across the entire corpus.<p>\n",
    "\n",
    "Now, let's implement these vectorization techniques using SKlearn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73462678",
   "metadata": {},
   "source": [
    "<h4> Bag-of-Words(Bow) Representes</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9835b343",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "595f182e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom tokenizer function for preprocessing\n",
    "\n",
    "def custom_tokenizer(text):\n",
    "    # Tokenization and lowercasing using spaCy\n",
    "    doc = nlp(text.lower())\n",
    "    # Stop word, punctuation  removal and lemmatization\n",
    "    tokens = [token.lemma_ for token in doc if not token.is_stop and not token.is_punct]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5759679b",
   "metadata": {},
   "source": [
    "<h5> <a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html\">Vectorizer</a></h5> \n",
    "<p>\"Convert a collection of text documents to a matrix of token counts\"</br>\n",
    "\"It transform a given text into a vector on the basis of the \n",
    " frequency of each word in the text\" </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9f36fd27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize CountVectorizer with custom tokenizer\n",
    "vectorizer = CountVectorizer(tokenizer=custom_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0f44f89d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example sentences\n",
    "sentences = [\"I love learning NLP.\", \"NLP is a technique to analyze natural languages. It is fun to learn.\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3f544276",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit and transform the sentences\n",
    "X_bow = vectorizer.fit_transform(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fac661f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the Bag-of-Words representation\n",
    "bow_representation = X_bow.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "61099fa6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Vocabulary:\n",
      "\n",
      "['analys' 'fun' 'languaes' 'learn' 'love' 'natural' 'nlp' 'technique']\n"
     ]
    }
   ],
   "source": [
    "# Print vocabulary\n",
    "print(\"\\nVocabulary:\\n\")\n",
    "print(vectorizer.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e6773ad3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bag-of-Words Representation:\n",
      "\n",
      "[[0 0 0 1 1 0 1 0]\n",
      " [1 1 1 1 0 1 1 1]]\n"
     ]
    }
   ],
   "source": [
    "# Print Bag-of-Words representation\n",
    "print(\"Bag-of-Words Representation:\\n\")\n",
    "print(bow_representation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f742a4d",
   "metadata": {},
   "source": [
    "<h3> TF-IDF (Term Frequency-Inverse Document Frequency) </h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5ba5d65",
   "metadata": {},
   "source": [
    "<p>It aims to reflect the importance of a word in a document relative to a collection of documents (corpus). TF-IDF combines two components:\n",
    "    \n",
    "<ul>\n",
    " <li><b> Term Frequency (TF)</b> Measures the frequency of a term (word) in a document.   It indicates how often a term occurs in a document relative to the total number of terms in the document. </li><br>\n",
    "    \n",
    "   <li><b>Inverse Document Frequency (IDF) </b> Measures the importance of a term across multiple documents in the corpus. It indicates how unique or rare a term is across the entire corpus. </li>\n",
    "</ul></p>\n",
    " <p><h5> The Forumula is </h5> \n",
    " \n",
    "TF-IDF(t,d)=TF(t,d)Ã—IDF(t).\n",
    "    \n",
    " where: \n",
    " \n",
    "Tf(t,d) = Is terem frequecny of term t in document d. \n",
    "Idf(t)  = Inverse dcoment frequency of term t in corpus. \n",
    "  \n",
    "<i>to calculate</i>\n",
    "     \n",
    "Tf(t,d) = number of occrance of terem t in document d / total number of termes in document d. \n",
    "Idf(t) = total number of documents in the corpus / number of documents containing term t. \n",
    "    \n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5c49e442",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "a94edaa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example corpus\n",
    "corpus = [\n",
    "    \"I love learning NLP.\",\n",
    "    \"NLP is a technique to analyze natural languages. It is fun to learn.\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "eaf162ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# intialize Tfidf Vectorizer  with custom tokenizer\n",
    "tf_idf_vectorizer = TfidfVectorizer(tokenizer = custom_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "405743da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit and transform the corpus with\n",
    "X_tfidf = tf_idf_vectorizer.fit_transform(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "027bbacc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Tfidf Vocabulary\n",
    "tfidf_vocabulary = tf_idf_vectorizer.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "7f94abdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Tfidf representation \n",
    "tfidf_representaion = X_tfidf.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "0c335b02",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I love learning NLP.',\n",
       " 'NLP is a technique to analyze natural languages. It is fun to learn.']"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print corpus\n",
    "corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "7d6bdd5e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['analyze', 'fun', 'language', 'learn', 'love', 'natural', 'nlp',\n",
       "       'technique'], dtype=object)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Get vocabulary \n",
    "tfidf_vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "cb473b3e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.        , 0.        , 0.50154891, 0.70490949,\n",
       "        0.        , 0.50154891, 0.        ],\n",
       "       [0.4078241 , 0.4078241 , 0.4078241 , 0.29017021, 0.        ,\n",
       "        0.4078241 , 0.29017021, 0.4078241 ]])"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get tfidf representation of corpus\n",
    "tfidf_representaion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c2a5e0a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
